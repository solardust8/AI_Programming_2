# Классификация видео при помощи VideoMAE
## Архитектура

Чтобы сделать предварительное обучение видео классификаторов более эффективным, в статье https://arxiv.org/abs/2203.12602 авторы представили архитектуру VideoMAE. Метод представляет из себя решение задачи самообучения с применением ассиметричной архитектуры энкодер-декодер и основан на частичном восстановлении входного видео. Для избавления от временной избыточности авторы используют чрезвычайно высокий коэффициент маскировки, чтобы удалять "кубы" (патчи кадра, помноженные на число кадров) из клипов с пониженным разрешением. Эта простая стратегия не только эффективно увеличивает производительность предварительного обучения, но и значительно снижает вычислительные затраты. В свою очередь, для учета временной корреляции, бэкбоун ViT утилизирует механизм joint space-time attention. Благодаря этой простой, но эффективной конструкции VideoMAE может успешно обучатьcя относительно небольших видео датасетах (Something-Something,  UCF101, HMDB51).
## VideoMAE
![Architecture](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/videomae_architecture.jpeg)
VideoMAE принимает уменьшенные клипы в качестве входных данных и извлекает "кубы" эмбеддинги для получения видеотокенов. Затем применяется tube masking с высоким коэффициентом для выполнения предварительного обучения MAE с асимметричной архитектурой энкодер-декодер.
## ViT Backbone
![](https://viso.ai/wp-content/uploads/2021/09/vision-transformer-vit.png)
В качестве бэкбоуна используется стандартный ViT c пространственно временным вниманием (благодаря временному измерению эмбеддинги превращаются в "куб" или "трубу", отсюда и названия)
## Joint space-time attention
![Joint Space-time attention](https://pic2.zhimg.com/80/v2-452a664874869897287fa1ef75b88661_720w.webp)


## Результаты работы (fine tuning)
В ходе данной работы было произведено дообучение предобученной модели VideoMAE (на базе Hugging Face transformers) c применением и без применения аугментации CutMix. Дообучение производилось на подмножестве датасета UCF101. 
### Training parameters
Дообучения происходило на онлайн-платформе Kaggle с применением вирутальной машины, располагающей ресурсами двух NVidia T4 GPU. 
В качестве оптимизатора был применен Adam со стандартными коэффициентами, начальным learning rate 0.0004, а так же линейным планировщиком. Дообучение происходило на протяжении 2 эпох в обоих случаях.
### Без CutMix
![loss](https://i.postimg.cc/BQmVXXwV/sample.gif)
Без аугментации задача сводится к стандартной задаче классификации, при которой на вход модели подается сам клип и соответстувующий ему класс (числом или onehot вектор с единственной единицей).
Результаты дообучения можно видеть в таблице ниже:
![loss](https://i.postimg.cc/Bbnvsm66/Nomix.png)

### CutMix
![loss](https://i.postimg.cc/RhMg0B1m/cutmix.gif)

Данная агументация состоит в врезании фрагмента одного изображения и замещения им фрагмента другого. CutMix служит для регуляризации модели, а так же помогает в получении более устойчивых признаков в ситуациях, когда в изображении сочитается разный контент, на основании чего ему можно присвоить несколько классов (лейблов) или сложно присвоить 1.
В результате аугментации вход модели перестает быть чётким, ведь теперь изображение (в нашем случае видео) теперь имеет 2 лейбла в разном соотношении (считается по площади). Таким образом, с точки зрения лейблов вход модели становится нечетким, ведь теперь это "onehot" вектор с двумя ненулевыми компонентами (соответствующими номерам классов), сумма которых дает 1.

Результат обучения можно видеть на графиках ниже:
![loss](https://i.postimg.cc/XJnhDhHr/Cutmix.png)

## Вывод по работе
Как можно видеть, при дообучении без аугментации модель довольно быстро сходится и уже после 7 эпох имеет точность на трейне 91% на тесте 89%. Это неудивительно, так как этот датасет участовал в экспериментах в оригинальной статье.

Однако же, модель начинает сильно расходится при добавлении нечеткого входа вместе с аугментированными клипами - точность и ошибка начинают колебаться без видимой сходимости. Ситуация не меняется и при большем числе эпох, и при других установках обучения (в частности, планировщика). Происходит это вероятно потому, что модель (в т.ч. ViT-В бэк) изначально предобучена на четких входах (ImageNET-1K 1600 эпох), после чего рутина предобучения VideoMAE так же не содержала нечетких входов.

## Источники
    VideoMAE: https://arxiv.org/pdf/2203.12602.pdf
    Space-time attention: https://arxiv.org/pdf/2102.05095.pdf
    ViT: https://arxiv.org/abs/2010.11929
    CutMix: https://arxiv.org/abs/1905.04899
    Pytorch: https://pytorch.org/


